{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dyi-el/optimized-traffic-light-computer-vision/blob/main/notebooks/vit-train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szWwJmqPHZ-r"
      },
      "source": [
        "## Training the Vision Transformer on a Custom Dataset\n",
        "\n",
        "In this notebook, we are going to fine-tune a pre-trained Vision Transformer (which can be found from [Huggingface](https://github.com/huggingface/transformers)) on a Custom Dataset. This notebook is based on Huggingface's [Fine tuning the Vision Transformer on CIFAR 10 notebook](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_the_%F0%9F%A4%97_Trainer.ipynb).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MCTZFkFw6i6",
        "outputId": "2458e38b-c98b-4562-c616-31d73d18ee9e"
      },
      "source": [
        "!pip install -q git+https://github.com/huggingface/transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZMQIvJDicO9"
      },
      "source": [
        "# Download the Data\n",
        "\n",
        "We'll preprocess and download our dataset from Roboflow. To preprocess the images, change the size of the image to 224x224. To download the dataset, use the \"**Folder Structure**\" export format."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"\")\n",
        "project = rf.workspace(\"dyiel\").project(\"traffic-density-4k\")\n",
        "version = project.version(2)\n",
        "dataset = version.download(\"folder\")"
      ],
      "metadata": {
        "id": "2iqIe5nu1Nmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XC9HqG5u750_"
      },
      "source": [
        "!unzip traffic-density.zip; rm traffic-density.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DSSAj4Os-Od"
      },
      "source": [
        "Next, convert the folder structure dataset into a PyTorch dataset format using PyTorch's ImageFolder dataset structure:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgn-a3NiSjqR"
      },
      "source": [
        "import torchvision\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "train_ds = torchvision.datasets.ImageFolder('/content/traffic-density-4k-2/train/', transform=ToTensor())\n",
        "valid_ds = torchvision.datasets.ImageFolder('/content/traffic-density-4k-2/valid/', transform=ToTensor())\n",
        "test_ds = torchvision.datasets.ImageFolder('/content/traffic-density-4k-2/test', transform=ToTensor())"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyHJjDYLfoFy"
      },
      "source": [
        "## Define the Model\n",
        "\n",
        "Here we define the model.\n",
        "\n",
        "The model itself uses a linear layer on top of a pre-trained `ViTModel`. We place a linear layer on top of the last hidden state of the [CLS] token, which serves as a good representation of an entire image. We also add dropout for regularization.\n",
        "\n",
        "**Note:** The Vision Transformer pretrained model can be used as a regular PyTorch layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGDrb1Q4ToLN"
      },
      "source": [
        "from transformers import ViTModel\n",
        "from transformers.modeling_outputs import SequenceClassifierOutput\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ViTForImageClassification(nn.Module):\n",
        "    def __init__(self, num_labels=3):\n",
        "        super(ViTForImageClassification, self).__init__()\n",
        "        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(self.vit.config.hidden_size, num_labels)\n",
        "        self.num_labels = num_labels\n",
        "\n",
        "    def forward(self, pixel_values, labels):\n",
        "        outputs = self.vit(pixel_values=pixel_values)\n",
        "        output = self.dropout(outputs.last_hidden_state[:,0])\n",
        "        logits = self.classifier(output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "          loss_fct = nn.CrossEntropyLoss()\n",
        "          loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "        if loss is not None:\n",
        "          return logits, loss.item()\n",
        "        else:\n",
        "          return logits, None"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BePiLK-LtXuG"
      },
      "source": [
        "## Define the Model Parameters\n",
        "\n",
        "To train this model, we will train in 3 epochs, with a batch size of 10 and a learning rate of 2e-5:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntGvS0_wUAxc"
      },
      "source": [
        "EPOCHS = 10\n",
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE = 2e-5"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "At9H-QOStt8_"
      },
      "source": [
        "We will use the pretrained Vision Transformer feature extractor, an Adam Optimizer, and a Cross Entropy Loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIyJr8EDtvlR"
      },
      "source": [
        "from transformers import ViTFeatureExtractor\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "# Define Model\n",
        "model = ViTForImageClassification(len(train_ds.classes))\n",
        "# Feature Extractor\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "# Adam Optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "# Cross Entropy Loss\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "# Use GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTZ-BP1yu1Us"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ql2T5PDUI1D"
      },
      "source": [
        "import torch.utils.data as data\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "\n",
        "print(\"Number of train samples: \", len(train_ds))\n",
        "print(\"Number of test samples: \", len(test_ds))\n",
        "print(\"Detected Classes are: \", train_ds.class_to_idx)\n",
        "\n",
        "train_loader = data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=4)\n",
        "test_loader  = data.DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(EPOCHS):\n",
        "  for step, (x, y) in enumerate(train_loader):\n",
        "    # Change input array into list with each batch being one element\n",
        "    x = np.split(np.squeeze(np.array(x)), BATCH_SIZE)\n",
        "    # Remove unecessary dimension\n",
        "    for index, array in enumerate(x):\n",
        "      x[index] = np.squeeze(array)\n",
        "    # Apply feature extractor, stack back into 1 tensor and then convert to tensor\n",
        "    x = torch.tensor(np.stack(feature_extractor(x)['pixel_values'], axis=0))\n",
        "    # Send to GPU if available\n",
        "    x, y  = x.to(device), y.to(device)\n",
        "    b_x = Variable(x)   # batch x (image)\n",
        "    b_y = Variable(y)   # batch y (target)\n",
        "    # Feed through model\n",
        "    output, loss = model(b_x, None)\n",
        "    # Calculate loss\n",
        "    if loss is None:\n",
        "      loss = loss_func(output, b_y)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    if step % 50 == 0:\n",
        "      # Get the next batch for testing purposes\n",
        "      test = next(iter(test_loader))\n",
        "      test_x = test[0]\n",
        "      # Reshape and get feature matrices as needed\n",
        "      test_x = np.split(np.squeeze(np.array(test_x)), BATCH_SIZE)\n",
        "      for index, array in enumerate(test_x):\n",
        "        test_x[index] = np.squeeze(array)\n",
        "      test_x = torch.tensor(np.stack(feature_extractor(test_x)['pixel_values'], axis=0))\n",
        "      # Send to appropirate computing device\n",
        "      test_x = test_x.to(device)\n",
        "      test_y = test[1].to(device)\n",
        "      # Get output (+ respective class) and compare to target\n",
        "      test_output, loss = model(test_x, test_y)\n",
        "      test_output = test_output.argmax(1)\n",
        "      # Calculate Accuracy\n",
        "      accuracy = (test_output == test_y).sum().item() / BATCH_SIZE\n",
        "      print('Epoch: ', epoch, '| train loss: %.4f' % loss, '| test accuracy: %.2f' % accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWXvWiB-srBC"
      },
      "source": [
        "## Evaluate on a Test Image\n",
        "\n",
        "Finally, let's evaluate the model on a test image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLv_xdYssuGO"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "EVAL_BATCH = 1\n",
        "eval_loader  = data.DataLoader(valid_ds, batch_size=EVAL_BATCH, shuffle=True, num_workers=4)\n",
        "# Disable grad\n",
        "with torch.no_grad():\n",
        "\n",
        "  inputs, target = next(iter(eval_loader))\n",
        "  # Reshape and get feature matrices as needed\n",
        "  print(inputs.shape)\n",
        "  inputs = inputs[0].permute(1, 2, 0)\n",
        "  # Save original Input\n",
        "  originalInput = inputs\n",
        "  for index, array in enumerate(inputs):\n",
        "    inputs[index] = np.squeeze(array)\n",
        "  inputs = torch.tensor(np.stack(feature_extractor(inputs)['pixel_values'], axis=0))\n",
        "\n",
        "  # Send to appropriate computing device\n",
        "  inputs = inputs.to(device)\n",
        "  target = target.to(device)\n",
        "\n",
        "  # Generate prediction\n",
        "  prediction, loss = model(inputs, target)\n",
        "\n",
        "  # Predicted class value using argmax\n",
        "  predicted_class = np.argmax(prediction.cpu())\n",
        "  value_predicted = list(valid_ds.class_to_idx.keys())[list(valid_ds.class_to_idx.values()).index(predicted_class)]\n",
        "  value_target = list(valid_ds.class_to_idx.keys())[list(valid_ds.class_to_idx.values()).index(target)]\n",
        "\n",
        "  # Show result\n",
        "  plt.imshow(originalInput)\n",
        "  plt.xlim(224,0)\n",
        "  plt.ylim(224,0)\n",
        "  plt.title(f'Prediction: {value_predicted} - Actual target: {value_target}')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGVIM5jX2Z5b"
      },
      "source": [
        "## Save the Entire Model\n",
        "\n",
        "We can save the entire model as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4S-BcSI2v88"
      },
      "source": [
        "torch.save(model, '/content/model.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeJ99BfV9QRo"
      },
      "source": [
        "## Export Trained Model\n",
        "\n",
        "Now that you have trained your custom vision transformer, you can export the trained model you have made here for inference on your device elsewhere"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFmfiH0Z3QJb",
        "outputId": "2351b3dd-4ced-4300-f13c-18c740b12a11"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "%cp /content/model.pt /content/gdrive/My\\ Drive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORZKpeH1REHP"
      },
      "source": [
        "## Use your Exported Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rS706FbORDiO"
      },
      "source": [
        "MODEL_PATH = '/content/model.pt'\n",
        "model = torch.load(MODEL_PATH)\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}